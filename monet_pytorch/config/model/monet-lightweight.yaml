# @package _global_

optimizer:
  _target_: torch.optim.rmsprop.RMSprop
  centered: true
  lr: 0.0001

model:
  _target_: monet_pytorch.model.Monet
  height: ${dataset.height}
  width: ${dataset.width}
  num_slots: ${dataset.max_num_objects}
  name: monet-lightweight
  bg_sigma: 0.10
  fg_sigma: 0.14
  beta_kl: 0.5
  gamma: 0.5
  input_channels: 3
  latent_size: 16
  encoder:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: monet_pytorch.template.sequential_cnn.make_sequential_cnn_from_config
        channels: [16, 16, 32, 32]
        kernels: 3
        strides: 2
        paddings: 0
        input_channels: 4
        batchnorms: true
        bn_affines: false
        activations: relu
      - _target_: torch.nn.Flatten
        start_dim: 1
      - _target_: torch.nn.Linear
        in_features: 64
        out_features: 64
      - _target_: torch.nn.ReLU
      - _target_: torch.nn.Linear
        in_features: 256
        out_features: ${prod:${model.latent_size},2}
  decoder:
    _target_: monet_pytorch.template.encoder_decoder.BroadcastDecoderNet
    w_broadcast: ${sum:${dataset.width},4}
    h_broadcast: ${sum:${dataset.height},4}
    net:
      _target_: monet_pytorch.template.sequential_cnn.make_sequential_cnn_from_config
      input_channels: 18 # latent size + 2
      channels: [16, 16, 4]  # last is 4 channels because rgb (3) + mask (1)
      kernels: [3, 3, 1]
      paddings: 0
      activations: [relu, relu, null]  #null means no activation function no activation
      batchnorms: [true, true, false]
      bn_affines: [false, false, false]
  unet:
    _target_: monet_pytorch.unet.UNet
    input_channels: ${model.input_channels}
    num_blocks: 5
    filter_start: 8
    mlp_size: 32